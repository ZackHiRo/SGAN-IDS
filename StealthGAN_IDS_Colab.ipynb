{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StealthGAN-IDS Training on Google Colab\n",
    "\n",
    "This notebook provides a complete pipeline for training and evaluating StealthGAN-IDS on Google Colab.\n",
    "\n",
    "**Features:**\n",
    "- Automatic GPU detection and setup\n",
    "- Dataset download and preprocessing\n",
    "- Training with checkpointing\n",
    "- Evaluation and visualization\n",
    "- Easy result download\n",
    "\n",
    "**Supported Datasets:**\n",
    "- NSL-KDD (legacy)\n",
    "- CIC-IDS2017\n",
    "- CIC-IDS2018\n",
    "- UNSW-NB15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "CUDA version: 12.8\n",
      "GPU Memory: 33.66 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected! Training will be very slow on CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: /workspace\n",
      "Repository directory: /workspace/SGAN-IDS\n",
      "âœ… Repository found at /workspace/SGAN-IDS\n",
      "Already up to date.\n",
      "Working directory: /workspace/SGAN-IDS\n",
      "Repository root: /workspace/SGAN-IDS\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "# Auto-detect environment (Colab vs QuickPod vs local)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect base directory\n",
    "if Path(\"/workspace\").exists():\n",
    "    # QuickPod environment\n",
    "    base_dir = Path(\"/workspace\")\n",
    "elif Path(\"/content\").exists():\n",
    "    # Google Colab environment\n",
    "    base_dir = Path(\"/content\")\n",
    "else:\n",
    "    # Local or other environment\n",
    "    base_dir = Path.cwd()\n",
    "\n",
    "repo_dir = base_dir / \"SGAN-IDS\"\n",
    "repo_url = \"https://github.com/yourusername/SGAN-IDS.git\"  # âš ï¸ UPDATE THIS if cloning\n",
    "\n",
    "print(f\"Detected environment: {base_dir}\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "\n",
    "# Check if repo exists in current directory or base directory\n",
    "if repo_dir.exists():\n",
    "    print(f\"âœ… Repository found at {repo_dir}\")\n",
    "    os.chdir(repo_dir)\n",
    "    if (repo_dir / \".git\").exists():\n",
    "        !git pull\n",
    "elif Path(\"SGAN-IDS\").exists():\n",
    "    # Check if repo is in current working directory\n",
    "    repo_dir = Path(\"SGAN-IDS\").resolve()\n",
    "    print(f\"âœ… Repository found at {repo_dir}\")\n",
    "    os.chdir(repo_dir)\n",
    "elif Path.cwd().name == \"SGAN-IDS\":\n",
    "    # Already in the repo directory\n",
    "    repo_dir = Path.cwd()\n",
    "    print(f\"âœ… Already in repository directory: {repo_dir}\")\n",
    "else:\n",
    "    # Try to clone or use current directory\n",
    "    if repo_url != \"https://github.com/yourusername/SGAN-IDS.git\":\n",
    "        print(f\"Cloning repository to {repo_dir}...\")\n",
    "        !git clone {repo_url} {repo_dir}\n",
    "        os.chdir(repo_dir)\n",
    "    else:\n",
    "        # Use current directory as repo (for QuickPod where files are already there)\n",
    "        repo_dir = Path.cwd()\n",
    "        print(f\"âš ï¸  Using current directory as repository: {repo_dir}\")\n",
    "        print(\"If this is wrong, update repo_url above or ensure SGAN-IDS folder exists\")\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Repository root: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.9.1+cu128 already installed with CUDA support âœ“\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# Check if torch is already installed with CUDA support (Colab has it pre-installed)\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Installing PyTorch with CUDA support...\")\n",
    "    !pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "else:\n",
    "    print(f\"PyTorch {torch.__version__} already installed with CUDA support âœ“\")\n",
    "\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Optional: Install additional evaluation dependencies\n",
    "!pip install -q xgboost lightgbm pyyaml\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup\n",
    "\n",
    "Choose your dataset and download/prepare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset: cic_ids2018\n",
      "Data root: /workspace/data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detect data root based on environment\n",
    "if Path(\"/workspace\").exists():\n",
    "    DATA_ROOT = Path(\"/workspace/data\")  # QuickPod\n",
    "elif Path(\"/content\").exists():\n",
    "    DATA_ROOT = Path(\"/content/data\")  # Colab\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./data\")  # Local\n",
    "\n",
    "DATA_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DATASET = \"cic_ids2018\"  # Options: nsl_kdd, cic_ids2017, cic_ids2018, unsw_nb15, unified\n",
    "\n",
    "print(f\"Selected dataset: {DATASET}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To download CIC-IDS2018:\n",
      "1. Install Kaggle: !pip install kaggle\n",
      "2. Upload kaggle.json (from Kaggle account settings)\n",
      "3. Run: !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
      "4. Then run:\n",
      "   !kaggle datasets download -d solarmainframe/ids-intrusion-csv -p /workspace/data\n",
      "   !unzip -q /workspace/data/ids-intrusion-csv.zip -d /workspace/data/CIC-IDS2018\n",
      "\n",
      "âš ï¸  Or manually upload dataset files via Colab file browser\n"
     ]
    }
   ],
   "source": [
    "# Download CIC-IDS2018 from Kaggle (requires Kaggle API)\n",
    "# Option 1: Using Kaggle API (recommended)\n",
    "if DATASET == \"cic_ids2018\":\n",
    "    print(\"To download CIC-IDS2018:\")\n",
    "    print(\"1. Install Kaggle: !pip install kaggle\")\n",
    "    print(\"2. Upload kaggle.json (from Kaggle account settings)\")\n",
    "    print(\"3. Run: !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\")\n",
    "    print(\"4. Then run:\")\n",
    "    print(f\"   !kaggle datasets download -d solarmainframe/ids-intrusion-csv -p {DATA_ROOT}\")\n",
    "    print(f\"   !unzip -q {DATA_ROOT}/ids-intrusion-csv.zip -d {DATA_ROOT}/CIC-IDS2018\")\n",
    "    print(\"\\nâš ï¸  Or manually upload dataset files via Colab file browser\")\n",
    "\n",
    "# Option 2: Manual upload via Colab file browser\n",
    "# Files > Upload to session storage > Extract to DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /workspace/SGAN-IDS\n",
      "Repository: /workspace/SGAN-IDS\n"
     ]
    }
   ],
   "source": [
    "# Quick fix for QuickPod - set paths manually\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# QuickPod paths\n",
    "repo_dir = Path(\"/workspace/SGAN-IDS\")\n",
    "os.chdir(repo_dir)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Repository: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/solarmainframe/ids-intrusion-csv\n",
      "License(s): Attribution 4.0 International (CC BY 4.0)\n",
      "Downloading ids-intrusion-csv.zip to /workspace/data\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 1.27G/1.60G [00:00<00:00, 4.54GB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.60G/1.60G [00:00<00:00, 4.53GB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d solarmainframe/ids-intrusion-csv -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q {DATA_ROOT}/ids-intrusion-csv.zip -d {DATA_ROOT}/CIC-IDS2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "StealthGAN-IDS Hyperparameter Tuning with Optuna\n",
      "============================================================\n",
      "[tune] Using device: cuda\n",
      "\n",
      "[1/3] Loading data...\n",
      "[tune] Limiting dataset to 300000 samples\n",
      "[CIC-IDS2018] Early stop after 1 files, ~450000 rows\n",
      "[CIC-IDS2018] Sampling 300000 from 450000 rows\n",
      "[CIC-IDS2018] Loaded shape: (300000, 80)\n",
      "[DataForge] Loaded datasets: ['cic_ids2018']\n",
      "[DataForge] Dropped 294 rows with NaN/inf values\n",
      "[DataForge] Data shape after cleaning: (160627, 80)\n",
      "[DataForge] Number of classes: 3\n",
      "[DataForge] Classes: ['Benign', 'FTP-BruteForce', 'SSH-Bruteforce']\n",
      "[DataForge] Split sizes - Train: 112438, Val: 24094, Test: 24095\n",
      "[DataForge] Fitting transformers on training data...\n",
      "[DataForge] Transforming validation data...\n",
      "[DataForge] Transforming test data...\n",
      "[DataForge] Converting sparse to dense...\n",
      "[DataForge] Number of features after encoding: 18338\n",
      "[DataForge] Reducing dimensions from 18338 to 256 using TruncatedSVD...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python scripts/tune_optuna.py --data-root /workspace/data --dataset cic_ids2018 --n-trials 50 --max-samples 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Dataset cic_ids2018 will be preprocessed automatically during training\n",
      "Skipping standalone preprocessing step...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset (optional - training will preprocess if needed)\n",
    "# Note: preprocess_data.py only supports: nsl_kdd, cic_ids2017, unified\n",
    "# For cic_ids2018 and unsw_nb15, preprocessing happens automatically during training\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Only preprocess if dataset is supported by preprocess script\n",
    "if DATASET in [\"nsl_kdd\", \"cic_ids2017\", \"unified\"]:\n",
    "    print(f\"Preprocessing {DATASET}...\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"scripts/preprocess_data.py\",\n",
    "        \"--data-root\", str(DATA_ROOT),\n",
    "        \"--dataset\", DATASET\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, cwd=repo_dir)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Preprocessing complete!\")\n",
    "    else:\n",
    "        print(f\"âŒ Preprocessing failed with exit code {result.returncode}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Dataset {DATASET} will be preprocessed automatically during training\")\n",
    "    print(\"Skipping standalone preprocessing step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Configure training parameters and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared. Available: 33.66 GB\n",
      "\n",
      "Training configuration:\n",
      "  Dataset: cic_ids2018\n",
      "  Batch size: 256\n",
      "  Max samples: 250000\n",
      "  Epochs: 100\n",
      "  Mixed precision: True\n",
      "  Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256  # Reduced for Colab memory (default is 256, use 64-128 for Colab)\n",
    "MAX_SAMPLES = 500000  # Limit dataset size for Colab RAM (None = use all, 500k works well)\n",
    "CHECKPOINT_INTERVAL = 5  # Save checkpoint every N epochs\n",
    "SEED = 42\n",
    "USE_AMP = True  # Mixed precision (faster on modern GPUs)\n",
    "NUM_WORKERS = 2  # Reduce if you get errors\n",
    "\n",
    "# Clear GPU memory before training\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Dataset: {DATASET}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES if MAX_SAMPLES else 'all'}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Mixed precision: {USE_AMP}\")\n",
    "print(f\"  Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Command: python scripts/train_gan.py --data-root /workspace/data --dataset cic_ids2018 --epochs 100 --batch-size 256 --checkpoint-interval 5 --seed 42 --num-workers 2 --max-samples 250000 --amp\n",
      "\n",
      "âš ï¸  This may take several hours. Checkpoints will be saved periodically.\n",
      "ðŸ’¡ If you get OOM errors, reduce MAX_SAMPLES (try 200000) or BATCH_SIZE (try 32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ’¡ If you get OOM errors, reduce MAX_SAMPLES (try 200000) or BATCH_SIZE (try 32)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"scripts/train_gan.py\",\n",
    "    \"--data-root\", str(DATA_ROOT),\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--epochs\", str(EPOCHS),\n",
    "    \"--batch-size\", str(BATCH_SIZE),\n",
    "    \"--checkpoint-interval\", str(CHECKPOINT_INTERVAL),\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--num-workers\", str(NUM_WORKERS),\n",
    "]\n",
    "\n",
    "if MAX_SAMPLES:\n",
    "    cmd.extend([\"--max-samples\", str(MAX_SAMPLES)])\n",
    "\n",
    "if USE_AMP:\n",
    "    cmd.append(\"--amp\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"\\nâš ï¸  This may take several hours. Checkpoints will be saved periodically.\")\n",
    "print(\"ðŸ’¡ If you get OOM errors, reduce MAX_SAMPLES (try 200000) or BATCH_SIZE (try 32)\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd, cwd=repo_dir)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ… Training completed successfully!\")\n",
    "elif result.returncode == -9:\n",
    "    print(\"\\nâŒ Training killed (exit code -9) - Out of Memory!\")\n",
    "    print(\"   Try reducing MAX_SAMPLES to 200000 or BATCH_SIZE to 32\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Training failed with exit code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Using device: cuda\n",
      "[train] Limiting dataset to 500000 samples\n",
      "[CIC-IDS2018] Early stop after 1 files, ~750000 rows\n",
      "[CIC-IDS2018] Sampling 500000 from 750000 rows\n",
      "[CIC-IDS2018] Loaded shape: (500000, 80)\n",
      "[DataForge] Loaded datasets: ['cic_ids2018']\n",
      "[DataForge] Dropped 1307 rows with NaN/inf values\n",
      "[DataForge] Data shape after cleaning: (359590, 80)\n",
      "[DataForge] Number of classes: 3\n",
      "[DataForge] Classes: ['Benign', 'FTP-BruteForce', 'SSH-Bruteforce']\n",
      "[DataForge] Split sizes - Train: 251712, Val: 53939, Test: 53939\n",
      "[DataForge] Fitting transformers on training data...\n",
      "[DataForge] Transforming validation data...\n",
      "[DataForge] Transforming test data...\n",
      "[DataForge] Converting sparse to dense...\n",
      "[DataForge] Number of features after encoding: 29093\n",
      "[DataForge] Reducing dimensions from 29093 to 256 using TruncatedSVD...\n",
      "Killed\n"
     ]
    }
   ],
   "source": [
    "!python scripts/train_gan.py --data-root /workspace/data --dataset cic_ids2018 --epochs 100 --checkpoint-interval 5 --seed 42 --num-workers 2 --max-samples 500000 --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training outputs:\n",
      "  âœ… training_stats.csv (0.00 MB)\n",
      "  âœ… generator_best.pth (4.61 MB)\n",
      "  âœ… generator_ema_best.pth (4.61 MB)\n",
      "  âœ… generator.pth (4.61 MB)\n",
      "  âœ… discriminator.pth (0.58 MB)\n",
      "\n",
      "Checkpoints found: 7\n",
      "  checkpoint_epoch_20.pth (20.19 MB)\n",
      "  checkpoint_epoch_25.pth (20.19 MB)\n",
      "  checkpoint_epoch_30.pth (20.19 MB)\n",
      "  checkpoint_epoch_35.pth (20.19 MB)\n",
      "  checkpoint_epoch_5.pth (20.19 MB)\n"
     ]
    }
   ],
   "source": [
    "# Check training outputs\n",
    "output_files = [\n",
    "    \"training_stats.csv\",\n",
    "    \"generator_best.pth\",\n",
    "    \"generator_ema_best.pth\",\n",
    "    \"generator.pth\",\n",
    "    \"discriminator.pth\",\n",
    "]\n",
    "\n",
    "print(\"Training outputs:\")\n",
    "for fname in output_files:\n",
    "    path = repo_dir / fname\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / 1e6\n",
    "        print(f\"  âœ… {fname} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {fname} (not found)\")\n",
    "\n",
    "# List checkpoints\n",
    "checkpoints = list(repo_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
    "if checkpoints:\n",
    "    print(f\"\\nCheckpoints found: {len(checkpoints)}\")\n",
    "    for cp in sorted(checkpoints)[-5:]:  # Show last 5\n",
    "        size_mb = cp.stat().st_size / 1e6\n",
    "        print(f\"  {cp.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Evaluate the trained generator with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation configuration:\n",
      "  Generator: generator_ema_best.pth\n",
      "  Samples per class: 2000\n",
      "  Target minority: True\n",
      "  CV folds: 5\n"
     ]
    }
   ],
   "source": [
    "# Evaluation configuration\n",
    "GENERATOR_PATH = \"generator_ema_best.pth\"  # Use EMA version (better quality)\n",
    "N_PER_CLASS = 2000  # Synthetic samples per class\n",
    "TARGET_MINORITY = True  # Focus on minority classes\n",
    "CV_FOLDS = 5  # Cross-validation folds\n",
    "OUTPUT_DIR = \"eval_outputs\"\n",
    "\n",
    "print(f\"Evaluation configuration:\")\n",
    "print(f\"  Generator: {GENERATOR_PATH}\")\n",
    "print(f\"  Samples per class: {N_PER_CLASS}\")\n",
    "print(f\"  Target minority: {TARGET_MINORITY}\")\n",
    "print(f\"  CV folds: {CV_FOLDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Command: python scripts/eval_gan.py --data-root /workspace/data --dataset cic_ids2018 --generator-path generator_ema_best.pth --n-per-class 2000 --cv-folds 5 --output-dir eval_outputs --max-samples 250000 --target-minority\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "import subprocess\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear memory before evaluation\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"scripts/eval_gan.py\",\n",
    "    \"--data-root\", str(DATA_ROOT),\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--generator-path\", GENERATOR_PATH,\n",
    "    \"--n-per-class\", str(N_PER_CLASS),\n",
    "    \"--cv-folds\", str(CV_FOLDS),\n",
    "    \"--output-dir\", OUTPUT_DIR,\n",
    "]\n",
    "\n",
    "# Pass max_samples to prevent OOM during data loading\n",
    "if MAX_SAMPLES:\n",
    "    cmd.extend([\"--max-samples\", str(MAX_SAMPLES)])\n",
    "\n",
    "if TARGET_MINORITY:\n",
    "    cmd.append(\"--target-minority\")\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "result = subprocess.run(cmd, cwd=repo_dir)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ… Evaluation completed successfully!\")\n",
    "elif result.returncode == -9:\n",
    "    print(\"\\nâŒ Evaluation killed (exit code -9) - Out of Memory!\")\n",
    "    print(\"   Try reducing MAX_SAMPLES to 200000 or N_PER_CLASS to 1000\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Evaluation failed with exit code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Results file not found. Run evaluation first.\n"
     ]
    }
   ],
   "source": [
    "# View evaluation results\n",
    "import json\n",
    "\n",
    "results_file = repo_dir / OUTPUT_DIR / \"evaluation_results.json\"\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"Evaluation Results Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if \"classifier_results\" in results:\n",
    "        print(\"\\nClassifier Performance:\")\n",
    "        for classifier, metrics in results[\"classifier_results\"].items():\n",
    "            if \"baseline\" in metrics and \"augmented\" in metrics:\n",
    "                baseline = metrics[\"baseline\"][\"mean\"]\n",
    "                augmented = metrics[\"augmented\"][\"mean\"]\n",
    "                improvement = augmented - baseline\n",
    "                print(f\"  {classifier}:\")\n",
    "                print(f\"    Baseline F1: {baseline:.4f}\")\n",
    "                print(f\"    Augmented F1: {augmented:.4f}\")\n",
    "                print(f\"    Improvement: {improvement:+.4f}\")\n",
    "    \n",
    "    if \"distribution_metrics\" in results:\n",
    "        print(\"\\nDistribution Quality:\")\n",
    "        for metric, value in results[\"distribution_metrics\"].items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Results file not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "View generated plots and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation plots\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plots_dir = repo_dir / OUTPUT_DIR / \"plots\"\n",
    "if plots_dir.exists():\n",
    "    plot_files = list(plots_dir.glob(\"*.png\"))\n",
    "    if plot_files:\n",
    "        print(f\"Found {len(plot_files)} plot(s):\")\n",
    "        for plot_file in plot_files:\n",
    "            print(f\"\\n{plot_file.name}:\")\n",
    "            display(Image(str(plot_file)))\n",
    "    else:\n",
    "        print(\"No plots found in plots directory.\")\n",
    "else:\n",
    "    print(\"Plots directory not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Results\n",
    "\n",
    "Download your trained models and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create download package\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "download_dir = Path(\"/content/downloads\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy important files\n",
    "files_to_download = [\n",
    "    \"generator_ema_best.pth\",\n",
    "    \"generator_best.pth\",\n",
    "    \"discriminator.pth\",\n",
    "    \"training_stats.csv\",\n",
    "]\n",
    "\n",
    "# Copy checkpoints\n",
    "checkpoints = list(repo_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
    "if checkpoints:\n",
    "    checkpoint_dir = download_dir / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    for cp in checkpoints:\n",
    "        shutil.copy2(cp, checkpoint_dir / cp.name)\n",
    "    print(f\"Copied {len(checkpoints)} checkpoints\")\n",
    "\n",
    "# Copy evaluation outputs\n",
    "eval_dir = repo_dir / OUTPUT_DIR\n",
    "if eval_dir.exists():\n",
    "    shutil.copytree(eval_dir, download_dir / OUTPUT_DIR, dirs_exist_ok=True)\n",
    "    print(\"Copied evaluation outputs\")\n",
    "\n",
    "# Copy files\n",
    "for fname in files_to_download:\n",
    "    src = repo_dir / fname\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, download_dir / fname)\n",
    "\n",
    "print(f\"\\nâœ… Files prepared for download in {download_dir}\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Use Colab file browser (left sidebar)\")\n",
    "print(\"2. Navigate to /content/downloads\")\n",
    "print(\"3. Right-click files and select 'Download'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create a zip file for easy download\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = \"/content/stealthgan_results.zip\"\n",
    "download_dir = Path(\"/content/downloads\")\n",
    "\n",
    "if download_dir.exists():\n",
    "    shutil.make_archive(\n",
    "        zip_path.replace(\".zip\", \"\"),\n",
    "        \"zip\",\n",
    "        download_dir\n",
    "    )\n",
    "    \n",
    "    size_mb = Path(zip_path).stat().st_size / 1e6\n",
    "    print(f\"âœ… Created zip file: {zip_path} ({size_mb:.2f} MB)\")\n",
    "    print(\"\\nDownloading zip file...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(\"âš ï¸  Download directory not found. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resume Training (Optional)\n",
    "\n",
    "Resume training from a checkpoint if your session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint\n",
    "CHECKPOINT_PATH = \"checkpoint_epoch_50.pth\"  # âš ï¸ Update with your checkpoint name\n",
    "RESUME_EPOCHS = 100  # Total epochs (will continue from checkpoint)\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"scripts/train_gan.py\",\n",
    "    \"--data-root\", str(DATA_ROOT),\n",
    "    \"--dataset\", DATASET,\n",
    "    \"--epochs\", str(RESUME_EPOCHS),\n",
    "    \"--batch-size\", str(BATCH_SIZE),\n",
    "    \"--resume\", CHECKPOINT_PATH,\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--num-workers\", str(NUM_WORKERS),\n",
    "]\n",
    "\n",
    "if MAX_SAMPLES:\n",
    "    cmd.extend([\"--max-samples\", str(MAX_SAMPLES)])\n",
    "\n",
    "if USE_AMP:\n",
    "    cmd.append(\"--amp\")\n",
    "\n",
    "print(\"To resume training, update CHECKPOINT_PATH above and uncomment the last line:\")\n",
    "print(f\"{' '.join(cmd)}\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# result = subprocess.run(cmd, cwd=repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "1. **Out of Memory (exit code -9)**:\n",
    "   - Reduce `MAX_SAMPLES` (try 200000 or 100000)\n",
    "   - Reduce `BATCH_SIZE` (try 32 or 16)\n",
    "   - Use a smaller dataset (`nsl_kdd` instead of `cic_ids2018`)\n",
    "2. **Session Timeout**: Colab free tier has 12hr limit. Use checkpoints to resume.\n",
    "3. **Dataset Not Found**: Ensure dataset is downloaded and extracted correctly.\n",
    "4. **Slow Training**: Enable GPU (Runtime > Change runtime type > GPU)\n",
    "\n",
    "**Memory Guide for Colab Free Tier (12GB RAM):**\n",
    "| Dataset | Recommended MAX_SAMPLES |\n",
    "|---------|------------------------|\n",
    "| NSL-KDD | None (all ~125K) |\n",
    "| CIC-IDS2017 | 500000 |\n",
    "| CIC-IDS2018 | 500000 |\n",
    "| UNSW-NB15 | None (all ~175K) |\n",
    "\n",
    "**Tips:**\n",
    "- Save checkpoints frequently\n",
    "- Use Colab Pro for longer sessions (24hr limit) and more RAM\n",
    "- Download results before session expires\n",
    "- Monitor GPU usage: `!nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
