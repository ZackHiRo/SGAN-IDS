{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StealthGAN-IDS Training on Google Colab\n",
        "\n",
        "This notebook provides a complete pipeline for training and evaluating StealthGAN-IDS on Google Colab.\n",
        "\n",
        "**Features:**\n",
        "- Automatic GPU detection and setup\n",
        "- Dataset download and preprocessing\n",
        "- Training with checkpointing\n",
        "- Evaluation and visualization\n",
        "- Easy result download\n",
        "\n",
        "**Supported Datasets:**\n",
        "- NSL-KDD (legacy)\n",
        "- CIC-IDS2017\n",
        "- CIC-IDS2018\n",
        "- UNSW-NB15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected! Training will be very slow on CPU.\")\n",
        "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "# Option 1: Clone from GitHub (update with your repo URL)\n",
        "repo_url = \"https://github.com/yourusername/SGAN-IDS.git\"  # âš ï¸ UPDATE THIS\n",
        "repo_dir = Path(\"/content/SGAN-IDS\")\n",
        "\n",
        "# Option 2: Upload repository as zip and extract\n",
        "# 1. Zip your repository locally\n",
        "# 2. Upload via Colab: Files > Upload\n",
        "# 3. Uncomment and run:\n",
        "# !unzip -q /content/SGAN-IDS.zip -d /content/\n",
        "\n",
        "if repo_dir.exists():\n",
        "    print(f\"Repository already exists at {repo_dir}\")\n",
        "    os.chdir(repo_dir)\n",
        "    # Try to pull updates if it's a git repo\n",
        "    if (repo_dir / \".git\").exists():\n",
        "        !git pull\n",
        "else:\n",
        "    if repo_url != \"https://github.com/yourusername/SGAN-IDS.git\":\n",
        "        !git clone {repo_url} {repo_dir}\n",
        "        os.chdir(repo_dir)\n",
        "    else:\n",
        "        print(\"âš ï¸  Please update repo_url with your repository URL\")\n",
        "        print(\"Or upload your repository as a zip file\")\n",
        "        print(\"\\nAlternative: Upload SGAN-IDS.zip and run:\")\n",
        "        print(\"  !unzip -q /content/SGAN-IDS.zip -d /content/\")\n",
        "        raise RuntimeError(\"Repository not configured - update repo_url or upload zip\")\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# Check if torch is already installed with CUDA support (Colab has it pre-installed)\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Installing PyTorch with CUDA support...\")\n",
        "    !pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "else:\n",
        "    print(f\"PyTorch {torch.__version__} already installed with CUDA support âœ“\")\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Optional: Install additional evaluation dependencies\n",
        "!pip install -q xgboost lightgbm pyyaml\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Setup\n",
        "\n",
        "Choose your dataset and download/prepare it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET = \"cic_ids2018\"  # Options: nsl_kdd, cic_ids2017, cic_ids2018, unsw_nb15, unified\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "DATA_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Selected dataset: {DATASET}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download CIC-IDS2018 from Kaggle (requires Kaggle API)\n",
        "# Option 1: Using Kaggle API (recommended)\n",
        "if DATASET == \"cic_ids2018\":\n",
        "    print(\"To download CIC-IDS2018:\")\n",
        "    print(\"1. Install Kaggle: !pip install kaggle\")\n",
        "    print(\"2. Upload kaggle.json (from Kaggle account settings)\")\n",
        "    print(\"3. Run: !mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\")\n",
        "    print(\"4. Then run:\")\n",
        "    print(f\"   !kaggle datasets download -d solarmainframe/ids-intrusion-csv -p {DATA_ROOT}\")\n",
        "    print(f\"   !unzip -q {DATA_ROOT}/ids-intrusion-csv.zip -d {DATA_ROOT}/CIC-IDS2018\")\n",
        "    print(\"\\nâš ï¸  Or manually upload dataset files via Colab file browser\")\n",
        "\n",
        "# Option 2: Manual upload via Colab file browser\n",
        "# Files > Upload to session storage > Extract to DATA_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the dataset (optional - training will preprocess if needed)\n",
        "# Note: preprocess_data.py only supports: nsl_kdd, cic_ids2017, unified\n",
        "# For cic_ids2018 and unsw_nb15, preprocessing happens automatically during training\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Only preprocess if dataset is supported by preprocess script\n",
        "if DATASET in [\"nsl_kdd\", \"cic_ids2017\", \"unified\"]:\n",
        "    print(f\"Preprocessing {DATASET}...\")\n",
        "    \n",
        "    cmd = [\n",
        "        \"python\", \"scripts/preprocess_data.py\",\n",
        "        \"--data-root\", str(DATA_ROOT),\n",
        "        \"--dataset\", DATASET\n",
        "    ]\n",
        "    \n",
        "    result = subprocess.run(cmd, cwd=repo_dir)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… Preprocessing complete!\")\n",
        "    else:\n",
        "        print(f\"âŒ Preprocessing failed with exit code {result.returncode}\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Dataset {DATASET} will be preprocessed automatically during training\")\n",
        "    print(\"Skipping standalone preprocessing step...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training\n",
        "\n",
        "Configure training parameters and start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64  # Reduced for Colab memory (default is 256, use 64-128 for Colab)\n",
        "MAX_SAMPLES = 500000  # Limit dataset size for Colab RAM (None = use all, 500k works well)\n",
        "CHECKPOINT_INTERVAL = 10  # Save checkpoint every N epochs\n",
        "SEED = 42\n",
        "USE_AMP = True  # Mixed precision (faster on modern GPUs)\n",
        "NUM_WORKERS = 2  # Reduce if you get errors\n",
        "\n",
        "# Clear GPU memory before training\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(f\"GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  Dataset: {DATASET}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Max samples: {MAX_SAMPLES if MAX_SAMPLES else 'all'}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Mixed precision: {USE_AMP}\")\n",
        "print(f\"  Seed: {SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "import subprocess\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"scripts/train_gan.py\",\n",
        "    \"--data-root\", str(DATA_ROOT),\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--epochs\", str(EPOCHS),\n",
        "    \"--batch-size\", str(BATCH_SIZE),\n",
        "    \"--checkpoint-interval\", str(CHECKPOINT_INTERVAL),\n",
        "    \"--seed\", str(SEED),\n",
        "    \"--num-workers\", str(NUM_WORKERS),\n",
        "]\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    cmd.extend([\"--max-samples\", str(MAX_SAMPLES)])\n",
        "\n",
        "if USE_AMP:\n",
        "    cmd.append(\"--amp\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"\\nâš ï¸  This may take several hours. Checkpoints will be saved periodically.\")\n",
        "print(\"ðŸ’¡ If you get OOM errors, reduce MAX_SAMPLES (try 200000) or BATCH_SIZE (try 32)\")\n",
        "\n",
        "# Run training\n",
        "result = subprocess.run(cmd, cwd=repo_dir)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\nâœ… Training completed successfully!\")\n",
        "elif result.returncode == -9:\n",
        "    print(\"\\nâŒ Training killed (exit code -9) - Out of Memory!\")\n",
        "    print(\"   Try reducing MAX_SAMPLES to 200000 or BATCH_SIZE to 32\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Training failed with exit code {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training outputs\n",
        "output_files = [\n",
        "    \"training_stats.csv\",\n",
        "    \"generator_best.pth\",\n",
        "    \"generator_ema_best.pth\",\n",
        "    \"generator.pth\",\n",
        "    \"discriminator.pth\",\n",
        "]\n",
        "\n",
        "print(\"Training outputs:\")\n",
        "for fname in output_files:\n",
        "    path = repo_dir / fname\n",
        "    if path.exists():\n",
        "        size_mb = path.stat().st_size / 1e6\n",
        "        print(f\"  âœ… {fname} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  âŒ {fname} (not found)\")\n",
        "\n",
        "# List checkpoints\n",
        "checkpoints = list(repo_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
        "if checkpoints:\n",
        "    print(f\"\\nCheckpoints found: {len(checkpoints)}\")\n",
        "    for cp in sorted(checkpoints)[-5:]:  # Show last 5\n",
        "        size_mb = cp.stat().st_size / 1e6\n",
        "        print(f\"  {cp.name} ({size_mb:.2f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation\n",
        "\n",
        "Evaluate the trained generator with comprehensive metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation configuration\n",
        "GENERATOR_PATH = \"generator_ema_best.pth\"  # Use EMA version (better quality)\n",
        "N_PER_CLASS = 2000  # Synthetic samples per class\n",
        "TARGET_MINORITY = True  # Focus on minority classes\n",
        "CV_FOLDS = 5  # Cross-validation folds\n",
        "OUTPUT_DIR = \"eval_outputs\"\n",
        "\n",
        "print(f\"Evaluation configuration:\")\n",
        "print(f\"  Generator: {GENERATOR_PATH}\")\n",
        "print(f\"  Samples per class: {N_PER_CLASS}\")\n",
        "print(f\"  Target minority: {TARGET_MINORITY}\")\n",
        "print(f\"  CV folds: {CV_FOLDS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "import subprocess\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"scripts/eval_gan.py\",\n",
        "    \"--data-root\", str(DATA_ROOT),\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--generator-path\", GENERATOR_PATH,\n",
        "    \"--n-per-class\", str(N_PER_CLASS),\n",
        "    \"--cv-folds\", str(CV_FOLDS),\n",
        "    \"--output-dir\", OUTPUT_DIR,\n",
        "]\n",
        "\n",
        "if TARGET_MINORITY:\n",
        "    cmd.append(\"--target-minority\")\n",
        "\n",
        "print(\"Starting evaluation...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "\n",
        "result = subprocess.run(cmd, cwd=repo_dir)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\nâœ… Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Evaluation failed with exit code {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View evaluation results\n",
        "import json\n",
        "\n",
        "results_file = repo_dir / OUTPUT_DIR / \"evaluation_results.json\"\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"Evaluation Results Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if \"classifier_results\" in results:\n",
        "        print(\"\\nClassifier Performance:\")\n",
        "        for classifier, metrics in results[\"classifier_results\"].items():\n",
        "            if \"baseline\" in metrics and \"augmented\" in metrics:\n",
        "                baseline = metrics[\"baseline\"][\"mean\"]\n",
        "                augmented = metrics[\"augmented\"][\"mean\"]\n",
        "                improvement = augmented - baseline\n",
        "                print(f\"  {classifier}:\")\n",
        "                print(f\"    Baseline F1: {baseline:.4f}\")\n",
        "                print(f\"    Augmented F1: {augmented:.4f}\")\n",
        "                print(f\"    Improvement: {improvement:+.4f}\")\n",
        "    \n",
        "    if \"distribution_metrics\" in results:\n",
        "        print(\"\\nDistribution Quality:\")\n",
        "        for metric, value in results[\"distribution_metrics\"].items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"âš ï¸  Results file not found. Run evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "View generated plots and visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display evaluation plots\n",
        "from IPython.display import Image, display\n",
        "\n",
        "plots_dir = repo_dir / OUTPUT_DIR / \"plots\"\n",
        "if plots_dir.exists():\n",
        "    plot_files = list(plots_dir.glob(\"*.png\"))\n",
        "    if plot_files:\n",
        "        print(f\"Found {len(plot_files)} plot(s):\")\n",
        "        for plot_file in plot_files:\n",
        "            print(f\"\\n{plot_file.name}:\")\n",
        "            display(Image(str(plot_file)))\n",
        "    else:\n",
        "        print(\"No plots found in plots directory.\")\n",
        "else:\n",
        "    print(\"Plots directory not found. Run evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Download Results\n",
        "\n",
        "Download your trained models and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create download package\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "download_dir = Path(\"/content/downloads\")\n",
        "download_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Copy important files\n",
        "files_to_download = [\n",
        "    \"generator_ema_best.pth\",\n",
        "    \"generator_best.pth\",\n",
        "    \"discriminator.pth\",\n",
        "    \"training_stats.csv\",\n",
        "]\n",
        "\n",
        "# Copy checkpoints\n",
        "checkpoints = list(repo_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
        "if checkpoints:\n",
        "    checkpoint_dir = download_dir / \"checkpoints\"\n",
        "    checkpoint_dir.mkdir(exist_ok=True)\n",
        "    for cp in checkpoints:\n",
        "        shutil.copy2(cp, checkpoint_dir / cp.name)\n",
        "    print(f\"Copied {len(checkpoints)} checkpoints\")\n",
        "\n",
        "# Copy evaluation outputs\n",
        "eval_dir = repo_dir / OUTPUT_DIR\n",
        "if eval_dir.exists():\n",
        "    shutil.copytree(eval_dir, download_dir / OUTPUT_DIR, dirs_exist_ok=True)\n",
        "    print(\"Copied evaluation outputs\")\n",
        "\n",
        "# Copy files\n",
        "for fname in files_to_download:\n",
        "    src = repo_dir / fname\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, download_dir / fname)\n",
        "\n",
        "print(f\"\\nâœ… Files prepared for download in {download_dir}\")\n",
        "print(\"\\nTo download:\")\n",
        "print(\"1. Use Colab file browser (left sidebar)\")\n",
        "print(\"2. Navigate to /content/downloads\")\n",
        "print(\"3. Right-click files and select 'Download'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Create a zip file for easy download\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "zip_path = \"/content/stealthgan_results.zip\"\n",
        "download_dir = Path(\"/content/downloads\")\n",
        "\n",
        "if download_dir.exists():\n",
        "    shutil.make_archive(\n",
        "        zip_path.replace(\".zip\", \"\"),\n",
        "        \"zip\",\n",
        "        download_dir\n",
        "    )\n",
        "    \n",
        "    size_mb = Path(zip_path).stat().st_size / 1e6\n",
        "    print(f\"âœ… Created zip file: {zip_path} ({size_mb:.2f} MB)\")\n",
        "    print(\"\\nDownloading zip file...\")\n",
        "    files.download(zip_path)\n",
        "else:\n",
        "    print(\"âš ï¸  Download directory not found. Run the previous cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Resume Training (Optional)\n",
        "\n",
        "Resume training from a checkpoint if your session disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume training from checkpoint\n",
        "CHECKPOINT_PATH = \"checkpoint_epoch_50.pth\"  # âš ï¸ Update with your checkpoint name\n",
        "RESUME_EPOCHS = 100  # Total epochs (will continue from checkpoint)\n",
        "\n",
        "cmd = [\n",
        "    \"python\", \"scripts/train_gan.py\",\n",
        "    \"--data-root\", str(DATA_ROOT),\n",
        "    \"--dataset\", DATASET,\n",
        "    \"--epochs\", str(RESUME_EPOCHS),\n",
        "    \"--batch-size\", str(BATCH_SIZE),\n",
        "    \"--resume\", CHECKPOINT_PATH,\n",
        "    \"--seed\", str(SEED),\n",
        "    \"--num-workers\", str(NUM_WORKERS),\n",
        "]\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    cmd.extend([\"--max-samples\", str(MAX_SAMPLES)])\n",
        "\n",
        "if USE_AMP:\n",
        "    cmd.append(\"--amp\")\n",
        "\n",
        "print(\"To resume training, update CHECKPOINT_PATH above and uncomment the last line:\")\n",
        "print(f\"{' '.join(cmd)}\")\n",
        "\n",
        "# Uncomment to run:\n",
        "# result = subprocess.run(cmd, cwd=repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "**Common Issues:**\n",
        "\n",
        "1. **Out of Memory (exit code -9)**:\n",
        "   - Reduce `MAX_SAMPLES` (try 200000 or 100000)\n",
        "   - Reduce `BATCH_SIZE` (try 32 or 16)\n",
        "   - Use a smaller dataset (`nsl_kdd` instead of `cic_ids2018`)\n",
        "2. **Session Timeout**: Colab free tier has 12hr limit. Use checkpoints to resume.\n",
        "3. **Dataset Not Found**: Ensure dataset is downloaded and extracted correctly.\n",
        "4. **Slow Training**: Enable GPU (Runtime > Change runtime type > GPU)\n",
        "\n",
        "**Memory Guide for Colab Free Tier (12GB RAM):**\n",
        "| Dataset | Recommended MAX_SAMPLES |\n",
        "|---------|------------------------|\n",
        "| NSL-KDD | None (all ~125K) |\n",
        "| CIC-IDS2017 | 500000 |\n",
        "| CIC-IDS2018 | 500000 |\n",
        "| UNSW-NB15 | None (all ~175K) |\n",
        "\n",
        "**Tips:**\n",
        "- Save checkpoints frequently\n",
        "- Use Colab Pro for longer sessions (24hr limit) and more RAM\n",
        "- Download results before session expires\n",
        "- Monitor GPU usage: `!nvidia-smi`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU usage\n",
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
